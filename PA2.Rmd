---
title: "Worst natural disaster threats in US"
output:
  pdf_document: default
  html_document:
    keep_md: yes
---

## Synopsis

In this report we aim to analise the natural disasters that pose the greatest threat to public health and economy of the United States. We will use the data from the [National Weather Service](http://www.weather.gov/), and will use the number of casualities and injured people to show the public health threat of the event, while using the property and crop looses in USD as a measure of the threat to the economy. If you need only an executive review of this document skip to the results section.


## Data Processing

Data may be obtained from the [National Weather Service](http://www.weather.gov/) and
[Storm Data Documentation](http://www.ncdc.noaa.gov/stormevents/) although data 
data used for the current analysis was provided from:

* [StormData.csv.bz2](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2)

There is also some documentation of the database available. Here you will find how some of the variables are constructed/defined.

* National Weather Service [Storm Data Documentation](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2Fpd01016005curr.pdf)
* National Climatic Data Center Storm Events [FAQ](https://d396qusza40orc.cloudfront.net/repdata%2Fpeer2_doc%2FNCDC%20Storm%20Events-FAQ%20Page.pdf)

### Software Environment information

This report was prepared using the following software environment.

```{r echo=TRUE}
sessionInfo()
```

### Used libraries

```{r echo=TRUE,message=FALSE}
library(dplyr)
library(R.oo)
library(ggplot2)
library(reshape2)
library(gridExtra)
```

### Loading and preprocessing the data

Raw data is extracted from it's [original distribution file](https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2).

```{r echo=TRUE, cache=TRUE}
raw_data <- read.csv(bzfile("StormData.csv.bz2"), na.strings="", stringsAsFactors=FALSE)
```

The data consists of `r dim(raw_data)[1]` rows of `r dim(raw_data)[2]`
variables.

Since we will be using years heavily throught the analysis, we'll add parsed
date and year variables.

```{r echo=TRUE, cache=TRUE}
dated_data <- mutate(raw_data, date=as.Date(raw_data$BGN_DATE,"%m/%d/%Y")) %>%
  mutate(year=as.integer(format(date, "%Y")))
```

### Exploratory information

The following graphs show the behavior of the types of events and event count for each year.

```{r echo=TRUE,fig.height=8, fig.width=8}
y_ev <- dated_data %>% group_by(year, EVTYPE) %>% summarise(count=n()) %>%
  summarise(count=sum(count),types=n())
par(mfrow = c(2,1))
plot(count ~ year, y_ev, main="Event count per year")
plot(types ~ year, y_ev, main="Types of events by year")
```

This shows that the count of event reports have been grown since 1950,
but after 1992 the grow rate is accelerated.

Also during 1993 the number of event types reported skyrocketed and then steadly
decreased until 2007 when it finally stabilized in 46 event types.

### Data cleanup

#### Making a usable dataset

Before 1993 only the following types of events were reported:

```{r echo=TRUE,results='asis'}
t <- dated_data %>% filter(year < 1993) %>% group_by(EVTYPE) %>% summarise()
knitr::kable( t, "Types of events reported before 1993", format="markdown" )
```

To avoid bias toward this type of events we'll delete data from years before 1993
and get a usable dataset.

```{r echo=TRUE}
use_data <- dated_data %>% filter(year >= 1993)
```

```{r echo=FALSE}
percent <- function (p) { sprintf("%.2f%%", 100*(p)) }
deleted_records <- dim(raw_data)[1] - dim(use_data)[1]
deleted_percent <- deleted_records / dim(raw_data)[1]
```

This deleted `r deleted_records` records which is about `r percent(deleted_percent)`
of the total number of raw records in the database

#### Event cleanup

Field EVTYPE has evident consistency issues, same labels appear multiple times
due to extra whitespace and different capitalization, a sample of this is shown.


```{r echo=TRUE}
raw_ev <- use_data %>% group_by(EVTYPE) %>% summarise(count=n())
raw_ev[c(grep("surf ad",raw_ev$EVTYPE,ignore.case = TRUE),grep("w.*y mix",raw_ev$EVTYPE,ignore.case = TRUE)), ]
```

Since the most obvious differences are whitespace and capitalization issues
we'll normalize the data by trimming extra whitespace and converting labels for
the events to uppercase.

```{r echo=TRUE,cache=TRUE}
# Get a cleaner event type by trimming and uppercasing
uc_ev <- data.frame(EVTYPE=as.factor(toupper(trim(raw_data$EVTYPE))))
uc_ev_sum <- uc_ev %>% group_by(EVTYPE) %>% summarise(count=n())
```

This reduced the number of different events from `r dim(raw_ev)[1]` to
`r dim(uc_ev_sum)[1]` (a `r percent(1-dim(uc_ev_sum)[1]/dim(raw_ev)[1])`).

Further manual adjustments were done by clustering similar events together by
assigning the same name, we also delete rare and nonsense events.

```{r echo=TRUE}
translator <- function (df) {
  misspellings = c(
    "FLOOODING", "FLOOD",
    "THUNDEERSTORM|THUNDEERSTORM|THUDERSTORM", "TSTM",
    "THUNDERSTORM", "TSTM",
    "TUNDERSTORM", "TSTM"
  )
  
  clusters <- c(
    "^COASTAL +FL", "COASTAL FLOOD",
    "^DRY MI(CR|RC)OBURST", "DRY MICROBURST",
    "EXTREME .*(COLD|CHILL)", "EXTREME COLD/WIND CHILL",
    "^FLASH FLOOD", "FLASH FLOOD",
    "^FLOOD", "FLOOD",
    "^(FROST|FREEZ)", "FROST/FREEZE",
    "^FUNNEL", "FUNNEL CLOUD",
    "^HAIL", "HAIL",
    "^HEAVY RAIN", "HEAVY RAIN",
    "^HEAVY SHOWER", "HEAVY RAIN",
    "^HEAVY PRECIP", "HEAVY RAIN",
    "^HEAVY .* SNOW", "HEAVY SNOW",
    "^HIGH WIND", "HIGH WIND",
    "^HURRICANE", "HURRICANE",
    "^IC(E|Y )", "ICE STORM",
    "^LAKE FLOOD", "LAKESHORE FLOOD",
    "^LAKE EFFECT SNOW", "LAKE-EFFECT SNOW",
    "^LIGHTNING", "LIGHTNING",
    "^MARINE .* WIND", "MARINE TSTM WIND",
    "^TORNADO", "TORNADO",
    "^TSTM WIND", "TSTM WIND",
    "^TROPICAL STORM", "TROPICAL STORM",
    "^WATERSPOUT", "WATERSPOUT",
    "WILD.*FIRE", "WILDFIRE",
    "^WINTER STORM", "WINTER STORM",
    "^WINTER WEATHER", "WINTER WEATHER",
    "WIN.*MIX", "WINTER WEATHER",
    "SURF", "HIGH SURF",
    "^RECORD LOW RAINFALL", "?",
    "(HEAVY|EXC|TORR).*RAIN", "HEAVY RAIN",
    "RAIN.*(HEAVY|EXC|TORR)", "HEAVY RAIN",
    "^SUMMARY", "?"
  )
  
  df <- df %>% mutate(label=EVTYPE);
  ms <- matrix(data=misspellings, ncol=2, byrow=TRUE)
  ms_num = dim(ms)[1]
  for (i in 1:ms_num) {
    df$label = gsub(ms[i,1],ms[i,2],df$label)
  }
  cs = matrix(data=clusters, ncol=2, byrow=TRUE)
  cs_num = dim(cs)[1] - 1
  for (row in 1:cs_num) {
    df$label[grep(cs[row,1], df$label)] = cs[row,2]
  }
  
  # Make label a factor
  df$label <- as.factor(df$label)
  
  # Delete nonsense events
  df <- df %>% filter(label != "?")
  
  # Delete rare event types
  df %>% group_by(label) %>% summarise(n=sum(count)) %>% filter(n > 10) %>%
    inner_join(df,by = "label") %>% select(label, EVTYPE)
}
```

The last transformation may be used to obtain a master translation table from
EVTYPE to labels.

```{r echo=TRUE}
rosetta <- translator(uc_ev_sum)
```

```{r echo=FALSE}
rosetta_labels <- length(levels(rosetta$label))
```

The translation table reduced the number of different events from
`r dim(uc_ev_sum)[1]` to `r rosetta_labels`, reducing the number of labels to
just the `r percent(rosetta_labels/dim(uc_ev_sum)[1])` of the normalized labels.

Using the master conversion table we may compute a clean event dataset with
unified lables and rare event types deleted.

```{r echo=TRUE}
clean_data <- inner_join(rosetta, use_data, by="EVTYPE")
```

```{r echo=FALSE}
deleted_records <- dim(use_data)[1] - dim(clean_data)[1]
deleted_percent <- deleted_records / dim(use_data)[1]
```

The event type cleanup was highly successful achieving to delete about
`r percent(rosetta_labels/dim(raw_ev)[1])`
of the original labels, by deleting most of the spurious labels, while preseving
the `r percent(1-deleted_percent)` of the usable dataset.

#### Exponent cleanup

Property and crop damage are expresed as numbers with magnitude factors, this
factors should be K, M, B standing for thousands, millions and billions
respectively, but the file has errors.

Property damages factors:
```{r echo=TRUE,results='asis'}
t <- summary(as.factor(clean_data$PROPDMGEXP))
knitr::kable(t, caption="Property damages factors", format="markdown",
             col.names=c("Count of Occurrences"))
```

There are a lot of missing factors, but most of them correspond to no damages as their cost is almost zero,
except for very few samples.

```{r echo=TRUE}
summary(cbind("Property Summary"=clean_data$PROPDMG[is.na(clean_data$PROPDMGEXP)]))
```

```{r echo=FALSE}
prop_unknown = length(grep("[^KkMmBb]",clean_data$PROPDMGEXP))
```

Also there are only `r prop_unknown` (`r percent(prop_unknown/length(clean_data$PROPDMGEXP))`)
unknown magnitudes for the property damages, so these samples can be safely
ignored.

Compute the cost vector for the property damages ignoring the cost for unknown 
and unavailable samples.

```{r echo=TRUE}
compute_exp <- function(numbers, magnitudes, xfactor) {
  get_factor <- function(magnitude) {
    if (is.na(magnitude)) xfactor
    else if (tolower(magnitude) == "k") 1e3
    else if (tolower(magnitude) == "m") 1e6
    else if (tolower(magnitude) == "b") 1e9
    else xfactor
  }
  numbers * sapply(magnitudes, get_factor)
}

prop_cost <- compute_exp(clean_data$PROPDMG, clean_data$PROPDMGEXP, 0)
```

Crop damage factors:
```{r echo=TRUE,results='asis'}
t <- summary(as.factor(clean_data$CROPDMGEXP))
knitr::kable(t, caption="Crop damages factors", format="markdown",
             col.names=c("Count of Occurrences"))
```

There are a lot of missing factors, but most of them correspond to no damages as their cost is almost zero,
except for very few samples.

```{r echo=TRUE}
summary(cbind("Crop Summary"=clean_data$CROPDMG[is.na(clean_data$CROPDMGEXP)]))
```

```{r echo=FALSE}
crop_unknown = length(grep("[^KkMmBb]",clean_data$CROPDMGEXP))
```

Also there are only `r crop_unknown` (`r percent(crop_unknown/length(clean_data$CROPDMGEXP))`)
unknown magnitudes for the crop damages, so these samples can be safely
ignored.

Compute the cost vector for the crop damages ignoring the cost for unknown 
and unavailable samples.

```{r echo=TRUE}
crop_cost <- compute_exp(clean_data$CROPDMG, clean_data$CROPDMGEXP, 0)
```


#### Working dataset

Then we compute the working dataset.

```{r echo=TRUE, cache=TRUE}
data <- clean_data %>% 
  rename(fatalities=FATALITIES, injuries=INJURIES, evtype=label) %>%
  mutate(propdmg = prop_cost) %>%
  mutate(cropdmg = crop_cost) %>%
  select(year, evtype, fatalities, injuries, propdmg, cropdmg)
```


## Results

```{r echo=FALSE}
top_num <- 20
```

Compute top `r top_num` causes of health and property damage.

```{r echo=TRUE}
sum_by_evType <- data %>% group_by(evtype) %>% 
    summarise(fatalities=sum(fatalities),injuries=sum(injuries),property=sum(propdmg),crop=sum(cropdmg))

top_health <- sum_by_evType %>%  top_n(top_num,fatalities)
top_econom <- sum_by_evType %>% top_n(top_num,property+crop)

top_x_sum <- sum_by_evType %>% summarise(fatalities=sum(fatalities),injuries=sum(injuries),property=sum(property),crop=sum(crop))

top_h_sum <- top_health %>%
  summarise(fatalities=sum(fatalities),injuries=sum(injuries),property=sum(property),crop=sum(crop))
top_h_pct <- top_h_sum / top_x_sum

top_e_sum <- top_econom %>%
  summarise(fatalities=sum(fatalities),injuries=sum(injuries),property=sum(property),crop=sum(crop))
top_e_pct <- top_e_sum / top_x_sum

num_years <- max(data$year) - min(data$year) + 1;
top_h_yr <- top_h_sum / num_years
top_e_yr <- top_e_sum / num_years
```

Top `r top_num` heath threats account for the `r percent(top_h_pct$fatalities)`
of all the fatalities, and the `r percent(top_h_pct$injuries)`
of all the injuries, causing the greatest impact in US population health with
anual means of `r round(top_h_yr$fatalities,digits = 0)` fatalities and
`r round(top_h_yr$injuries,digits = 0)` injuries.

Top `r top_num` economic threats account for the `r percent(top_e_pct$property)`
of all the property damages, and the `r percent(top_e_pct$crop)`
of all the crop damages, causing the great majority of the economic threats to
the US economy with anual property loses of `r round(top_e_yr$property/1e9,digits = 1)` billion
and crop loses of `r round(top_e_yr$crop/1e9,digits = 1)` billion.

The next chart shows the `r top_num` events that threaten the US public health 
and economy ranked by its impact.

```{r echo=TRUE}
health_rank <- function () {
  f <- top_health %>% select(evtype,fatalities,injuries)
  f$evtype <- reorder(f$evtype, f$injuries+f$fatalities)
  f <- melt(f,id.vars = "evtype")
  ggplot(data=f, aes(evtype, value)) + geom_bar(stat="identity",aes(fill=variable)) +
    coord_flip() + xlab('')+ ylab('Number of casualties') +
    ggtitle(sprintf("Top %d Causes of health thread", top_num))  
}

econom_rank <- function () {
  f <- top_econom %>% select(evtype,property,crop)
  f$evtype <- reorder(f$evtype, f$property+f$crop)
  f <- melt(f,id.vars = "evtype")
  f$value <- f$value / 1e9
  ggplot(data=f, aes(evtype, value)) + geom_bar(stat="identity",aes(fill=variable)) +
    coord_flip() + xlab('')+ ylab('Loses in billions of USD') +
    ggtitle(sprintf("Top %d Causes of economic damage",top_num))
}
```

```{r echo=TRUE, fig.height=10, fig.width=8}
grid.arrange(health_rank(), econom_rank(), ncol=1)
```

